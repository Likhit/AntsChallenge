<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <link rel="stylesheet" type="text/css" href="index.css">

    <script type="text/javascript" src="https://distill.pub/template.v2.js"></script>
    <script type="text/javascript" src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script src="index.js"></script>

    <title>Using Deep Reinforcement Learning to play Ants</title>
</head>
<body>
    <d-front-matter>
        <script type="text/json">{
            "title": "Using Deep Reinforcement Learning to play Ants",
            "description": "Ants is a multiplayer turn based capture-the-hill board game. Here we attempt to train a bot to play the game using deep RL.",
            "authors": [{
                "author": "Anurima Padwal",
                "authorURL": "https://www.linkedin.com/in/anurima-padwal-433260135/",
                "affiliation": "University of Southern California"
            }, {
                "author": "Apoorva Katharaki Hathi",
                "authorURL": "https://www.linkedin.com/in/apoorvakh/",
                "affiliation": "University of Southern California"
            }, {
                "author": "Arunkumar Rajendran",
                "authorURL": "https://www.linkedin.com/in/arunkumar-rajendran/",
                "affiliation": "University of Southern California"
            }, {
                "author": "Dhanya Raghu",
                "authorURL": "https://www.linkedin.com/in/dhanya-raghu/",
                "affiliation": "University of Southern California"
            }, {
                "author": "Likhit Dharmapuri",
                "authorURL": "https://www.linkedin.com/in/likhitd",
                "affiliation": "University of Southern California"
            }]
        }</script>

    </d-front-matter>

    <d-title>
        <h1>Using Deep Reinforcement Learning to Play Ants</h1>
        <p>Ants is a multiplayer turn based capture-the-hill board game. Here we attempt to train a bot to play the game using deep reinforcement learning.</p>
        <d-figure class="l-screen">
            <video id="poster-vid" src="assets/videos/winnergame.mp4" autoplay mute controls loop preload="auto" playbackRate="3"></video>
            <figcaption>
                An example of the ants game with 7 players. Each player starts with two ant hills in this game.
            </figcaption>
        </d-figure>
    </d-title>
    <d-article>
        <p>The <a href="http://ants.aichallenge.org/">“Ants” AI Challenge</a> <d-cite key="antschallenge"></d-cite> was an artificial intelligence programming contest conducted by the University of Waterloo in Fall 2011. This contest requires the creation of an AI agent that controls a colony of ants to fight other colonies for dominance in a competitive environment.</p>
        <p>The game itself is a multiplayer strategy game set on a plot of dirt and water. Each player starts with a set of ant hills and one ant on each hill. By gathering food that spawns randomly on the board, players can increase the number of ants under their control. The final objective of the game is to capture the enemy's ant hill by placing the player's ant on top of the enemy hill.</p>

        <h3>Game Rules</h3>
        <p><em>Ants</em> is a turn based game. Players can only see the portion of the board within the vicinity of their own ants. At each turn, every player submits moves each of their ants on the board in one of the four directions: north, east, south or west. Once all players have made their submissions the game executes the turn in the following phases:</p>

        <ol id="game-phases">
            <li><em>move</em> all ants in the specified direction (ants from the same team are killed if they collide)</li>

            <li><em>attack</em> enemy ants within range of each other</li>

            <li><em>raze</em> any ant hill with an enemy ant positioned on it</li>

            <li><em>spawn</em> more ants at un-razed player ant hills if the player has enough food reserves</li>

            <li><em>gather</em> food within range of an ant (food disappears if it is within range of multiple enemy ants)</li>
        </ol>

        <p>After execution of all phases, each player receives a new board state, and the game proceeds to the next turn.</p>

        <h3>Challenges</h3>
        <p>Since each ant on the board is moved at the end of every turn, the player is in effect controlling multiple agents (one agent per ant). Not only that, the number of agents is not fixed &mdash; as the game progresses, the number of ants on the board will increase. Multi-agent RL is a challenging problem which is still in its nascent stages of research.</p>
        <p>All entries to the competition so far have been implementations of min-max and the A* algorithm. There has been no attempt so far to use a trained bot to play the game.</p>
        <p>This allows us to work on something novel and challenging. At the same time the computationally simple nature of the game provides a feasible problem to work on.</p>

        <h2>Problem Formulation</h2>
        <p>For the sake of this project we limit ourselves to boards with only two players, and only one ant hill per player. We also limit ourselves to playing on the following two maps.</p>

        <d-figure>
            <div id="maps">
                <img src="assets/images/map1.png" alt="map1">
                <img src="assets/images/map2.png" alt="map2">
            </div>
            <figcaption>The maps used for both training and testing throughout this project. The left map is of size <code>43 x 39</code>, while the map on the right is of size <code>110 x 114</code>.</figcaption>
        </d-figure>

        <p>Henceforth, the map on the left will be called <code>Map1</code>, while the map on the right will be called <code>Map2</code> in the rest of this post.</p>

        <p>A common vocabulary used to define problems in the RL landscape are:</p>
        <ul>
            <li><em>State Space:</em> The set of all states that the game can take, and</li>
            <li><em>Action Space:</em> The set of all possible actions that can be ever taken</li>
        </ul>
        <p>This vocabulary is also used to describe problems abstractly by <a href="https://gym.openai.com/">OpenAI Gym</a>, the popular toolkit for developing and comparing reinforcement learning algorithms. Therefore, we formulated our game using the same vocabulary in order to leverage existing work on RL. We created an OpenAi Gym environment for the game as described below.</p>

        <h3>Game State</h3>
        <p>Given a game board of height <code>H</code> and width <code>W</code>, there are a number aspects on the board (like visible area, food positions, etc.) which the game state needs to capture. Using an image of the game board would be unnecessarily complicated. Instead, the game board can be described as a byte tensor <code>S</code> of the shape <code>(7, H, W)</code> where the seven channels describe different board features.</p>

        <d-figure class="l-page">
            <div id="state-diag">
                <img src="assets/images/full.png">
                <ul id="state-channels">
                    <li id="visibility-channel">Visibility Channel: <code>True</code> if the cell is visible to the agent</li>
                    <li id="land-channel">Land Channel: <code>True</code> if the the cell is water, <code>False</code> if land</li>
                    <li id="food-channel">Food Channel: <code>True</code> if the cell has food</li>
                    <li id="ant-hill-channel">Ant Hill Channel: <code>PLAYER NUMBER</code> if the cell contains an ant hill</li>
                    <li id="player-ant-channel">Player Ant Channel: <code>True</code> if the cell contains one of the player's ants</li>
                    <li id="enemy-ant-channel">Enemy Ant Channel: <code>True</code> if the cell contains one of the enemy's ants</li>
                </ul>
                <img id="channel-img" src="assets/images/visibility.png">
            </div>
            <figcaption>State description of example game board from the perspective of the orange player. Black color represents a value of <code>1</code> / <code>True</code>, while white depicts <code>0</code> / <code>False</code>; except in the visibility channel in which white depicts the visible cells.</figcaption>
        </d-figure>

        <p>Not depicted in the above visual is another channel which is <code>True/1</code> in cells in which the ants have died in the previous turn.</p>

        <h3>Action Space</h3>
        <p>In order to avoid dealing with a changing action space (which changes with the number of ants on the board), we instead use a constant action space tensor <code>A</code> of the shape <code>(H, W)</code>. The values in <code>A[h, w]</code> can be one of <code>north (1)</code>, <code>south (2)</code>, <code>east (3)</code>, <code>west (4)</code>, and <code>none (0)</code>. The values describe the direction in which the ant in cell <code>(h, w)</code> should move. <code>none (0)</code> is used in cells without any ant, and for ants which shouldn't move.</p>

        <d-figure>
            <img src="assets/images/actionspace.jpeg">
            <figcaption>An example action for the given board state of the orange player. Blue depicts <code>north</code>, pink depicts <code>south</code>, yellow <code>east</code> and green <code>west</code>. All the white cells (majority of the map) are <code>none</code></figcaption>
        </d-figure>

        <h3>Rewards</h3>
        <p>A reward of <code>1</code> is obtained every time an enemy ant hill is razed, and a penalty of <code>-1</code> is incurred when the player's ant hill is razed. Such a reward is very sparse since most games go on for a few hundred turns. In order to augment the score, we have developed a number of other reward functions.</p>

        <ol>
            <li><em>Food : </em> Rewards movement towards the food positively.</li>
            <li><em>Score : </em> Improvement in score is positively rewarded.</li>
            <li><em>Death : </em> Death of ants is negatively rewarded.</li>
            <li><em>Collision : </em> Collision of two ants of the same team is penalized.</li>
            <li><em>Obstacles : </em> Encourages ants to move away from obstacles (i.e water).</li>
        </ol>

        <p>The new reward functions all have the ability to provide rewards for each ant rather than the whole board state. Depending on the mode of training, we might choose to use this distributed reward, or the total state reward.</p>

        <h2>Approaches</h2>
        <p>We primarily attempted to train bots using multiple variations of the DQN and A2C algorithm. But before we detail the approaches, we must first specify a way to test the performance of our trained bots.</p>

        <h3>Baselines</h3>
        <p>In order to test the performance of our bots we played our bots against a number of simpler bots.</p>

        <ol>
            <li><em>Random bot:</em> The Random bot moves every ant a random direction every turn.</li>
            <li><em>Hunter bot:</em> The Hunter bot targets enemy ants and pursues them.</li>
            <li><em>Greedy bot:</em> The Greedy bot prioritizes moves in the order of hunting enemy hills over hunting for food over hunting enemy ants over exploring the map.</li>
            <li><em>Winner bot:</em> The Winner bot<d-cite key="winnerbot"></d-cite> is the winner bot of the Ants AI challenge held in 2011 </li>
        </ol>

        <d-figure class="l-screen">
            <video id="poster-vid" src="assets/videos/allbots2.mp4" autoplay mute controls loop preload="auto"></video>
            <figcaption>A game of each of the mentioned bots against each other. Yellow is the winner bot, orange is the random bot, pink is the hunter bot, and blue is the greedy bot.</figcaption>
        </d-figure>

        <p>In order to discount for randomness of the game (eg. food spawn rate, etc.) we run the trained bots multiple times against each one of the baseline bots.</p>

        <h3>DQN</h3>
        <p>Reinforcement Learning is a technique in which an agent attempts to learn an optimal policy based on its interactions with the environment. At each time step, the agent observes a state \(S\), chooses an action \(A\) to perform on the state, obtains a reward \(R\), and transitions to a new state \(\hat{S}\). Q-learning <d-cite key="qlearning"></d-cite><d-cite key="Mnih2015"></d-cite><d-cite key="DBLP:journals/corr/TampuuMKKKAAV15"></d-cite> is an approach in which we incrementally estimate the “value” of performing an action \(A\) in state \(S\) by updating the following rule:
            $$ Q(S, A) = Q(S, A) + \alpha \cdot (R + \gamma \cdot max_{\hat{A}} Q(\hat{S}, \hat{A}) - Q(S, A)) $$
        </p>

        <p>Here, \(Q(S, A)\) represents the quality of taking an action \(A\) in state \(S\). Q-learning can be extended to deep reinforcement learning frameworks by using a neural network as a function approximator \(Q(S, A|\theta)\) where \(\theta\) represents the weights of the network that parametrize the Q values.</p>

        <p>We use a multi-layer CNN network on the state space of the game to output the value of taking an action at each cell. Hence, the output of the \(Q\) function is a tensor of the shape <code>(5, H, W)</code>. Each channel \(i\) in the output tensor represents the value of taking action \(i\) at the cell. For exploration, a random action is chosen for each cell, while for exploitation the action with the highest value is chosen.</p>

        <d-figure>
            <img src="assets/images/DQN.jpg">
            <figcaption>The Q function network used</figcaption>
        </d-figure>

        <p>We use temporal difference learning <d-cite key="Sutton1988"></d-cite> with Huber loss to train the network.
            $$
            L(&delta;) =
            \begin{cases}
            \frac{1}{2} \cdot (Q(S, A) - (R + \gamma \cdot max_{\hat{A}} Q(\hat{S}, \hat{A})))^2 & \text{if } |(y - \hat{y})| &lt; &delta; \\
            &delta; \cdot (Q(S, A) - (R + \gamma \cdot max_{\hat{A}} Q(\hat{S}, \hat{A}))) - \frac{&delta;}{2} & \text{otherwise}
            \end{cases}
            $$
        </p>

        <p>Experience replay with a buffer size of \(10000\) is used to further improve the training of the model. Using the different reward functions we already created we trained a number of bots using combinations of different reward functions:</p>

        <ul>
            <li><em>Collision</em> bot uses only the collision reward.</li>
            <li><em>Die</em> bot uses only the reward from the death reward function.</li>
            <li><em>Aggregate</em> bot uses a linear combination of all reward functions.</li>
        </ul>

        <p>Due to problems with overestimation of Q-values when using a <code>max</code> operator over the possible actions, we also trained a Double DQN <d-cite key="DBLP:journals/corr/HasseltGS15"></d-cite> using the aggregate rewards.</p>

        <h4>Results</h4>
        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">AggregateRewards</th>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">84</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">121</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td>27</td>
                </tr>
                <tr>
                    <th class="right-border">Collision</th>
                    <td>0%</td>
                    <td>90%</td>
                    <td>10%</td>
                    <td class="right-border">108</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">73</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td>29</td>
                </tr>
                <tr>
                    <th class="right-border">Die</th>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">157</td>
                    <td>0%</td>
                    <td>80%</td>
                    <td>20%</td>
                    <td class="right-border">65</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td>37</td>
                </tr>
                <tr>
                    <th class="right-border">Double</th>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">988</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td class="right-border">386</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>0%</td>
                    <td>189</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained DQN bots against the baseline bots on <code>Map1</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>

        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">AggregateRewards</th>
                    <td>0%</td>
                    <td>30%</td>
                    <td>70%</td>
                    <td class="right-border">94</td>
                    <td>0%</td>
                    <td>30%</td>
                    <td>70%</td>
                    <td class="right-border">151</td>
                    <td>0%</td>
                    <td>30%</td>
                    <td>70%</td>
                    <td>151</td>
                </tr>
                <tr>
                    <th class="right-border">Collision</th>
                    <td>0%</td>
                    <td>20%</td>
                    <td>80%</td>
                    <td class="right-border">94</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td>80%</td>
                    <td class="right-border">145</td>
                    <td>0%</td>
                    <td>40%</td>
                    <td>60%</td>
                    <td>327</td>
                </tr>
                <tr>
                    <th class="right-border">Die</th>
                    <td>0%</td>
                    <td>20%</td>
                    <td>80%</td>
                    <td class="right-border">151</td>
                    <td>0%</td>
                    <td>70%</td>
                    <td>30%</td>
                    <td class="right-border">77</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td>100%</td>
                    <td>111</td>
                </tr>
                <tr>
                    <th class="right-border">Double</th>
                    <td>0%</td>
                    <td>20%</td>
                    <td>80%</td>
                    <td class="right-border">548</td>
                    <td>0%</td>
                    <td>60%</td>
                    <td>40%</td>
                    <td class="right-border">470</td>
                    <td>0%</td>
                    <td>40%</td>
                    <td>60%</td>
                    <td>1138</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained DQN bots against the baseline bots on <code>Map2</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>

        <d-figure class="l-page">
            <video src="assets/videos/dqn-compare2.mp4" autoplay mute controls loop preload="auto"></video>
            <figcaption>Comparison of the untrained DQN network (on left) and trained DQN network (right) vs the <code>GreedyBot</code> (blue player). Depicted is the <code>Affiliate</code> bot Through training we are able to go from a swift defeat to a major victory.</figcaption>
        </d-figure>

        <p>The results are tabulated for DQN architecture for different reward functions. Collision and die rewards individually perform better than the aggregate; as the agent will not be able to focus simultaneously on different rewards as they gets neutralized. The model performs best against random bot for both maps. Overall, our agents perform better on <code>Map2</code> because <code>Map2</code> is bigger than <code>Map1</code>, thus giving more area of land for the agent ants to explore and reducing the rate of collision amongst them.</p>

        <p>Given that we are also trying to learn the actions for all ants on the board in one shot, it is understandable that the DQN algorithm isn't performing well. The \(Q\) function described here is an extremely hard function to learn. For a board state with \(n\) ants, the network must find the optimal value combination out of a total of \(5^n\) possible combinations.</p>

        <h3>Supervised Training + A2C</h3>
        <p>The <em>A2C</em> or Advantage Actor Critic <d-cite key="DBLP:journals/corr/MnihBMGLHSK16"></d-cite> algorithms are a set of policy gradient methods. The agent has two parts, the <strong>actor</strong> i.e. the policy network which learns the probability with which each action should be taken, and the <strong>critic</strong> i.e. the value function which estimates the value of each state. The critic is used to compare against the actual reward obtained by the agent; the difference which is the error is then used to update both the actor as well as the critic.</p>

        <p>So far, we have been trying to learn the actions for all ants on the board in one shot. As described earlier, this is an extremely hard function to learn. In order to offset this problem, we instead try to train a network which learns the best action for each ant separately.</p>

        <p>In order to do so, given a game state \(S\), we decompose \(S\) into a sub-state \(s_i\) for each ant \(i\) in \(S\). This is done by taking a square centered around each any \(i\).</p>

        <d-figure>
            <img src="assets/images/substates.png">
            <figcaption>Sub-states created for the orange player. A small box of <code>15x15</code> cells is taken centered around each ant.</figcaption>
        </d-figure>

        <p>Once we have our sub states, we define our policy \(\pi(s_i, \theta_{\pi})\) which returns an action \(a_i\) for the ant \(i\). Similarly, our value function is defined as \(Q(s_i, a_i)\) which returns the reward \(r_i\) obtained by performing action \(a_i\) on ant \(i\).</p>

        <p>We attempt to learn the policy and value function using the following networks.</p>

        <d-figure>
            <div id="a2c-networks">
                <img src="assets/images/a2cpolicy-1.jpg">
                <img src="assets/images/a2cvalue-1.jpg">
            </div>
            <figcaption>The policy network (left), and the value network (right) used. The subsets created for training use a box of size <code>18 x 18</code>. The output includes both a score reward, and a food reward both tensors of size \(5\).</figcaption>
        </d-figure>

        <p>The training technique described in this section is heavily inspired from the AlphaGo paper <d-cite key="AlphaGo"></d-cite>. The training of the bot goes through two phases.</p>

        <h4>Supervised training</h4>
        <p>In this stage, we learn a policy and value function from generated training data. The training data is generated by playing the <code>RandomBot</code>, <code>HunterBot</code>, <code>GreedyBot</code>, and the <code>WinnerBot</code> against each other multiple times on <code>Map1</code>, and <code>Map2</code>. For each turn of every game, we store the current state \(S\), action taken \(A\), and reward earned \(R\) by each player. The total data collected is broken an \(80/20\) train/test split.</p>

        <h5>Policy network training</h5>
        <p>The network is trained only on samples generated by the <code>WinnerBot</code>. Given the state we decompose it into a set of sub-states, and try to predict the action taken taken by the winner bot for the ant in that sub-state. We use <code>cross-entropy</code> loss for the weight update.</p>

        <h5>Value network training</h5>
        <p>In order to train the value network, we use both the <code>FoodReward</code> as well as the game scores. We do not restrict the training data in any way. We obtain the game score for non-final turns by discounting the final score obtained in a gameplay by a factor or \(0.999\). This discounted score is called the turn score. For each sub-state of a state, the network tries to predict both the food gathered by the ant in the sub-state, as well as the turn score for the whole state. <d-footnote>We also tried to learn \(\frac{1}{N} \cdot turnscore\) (where \(N\) is number of ants) instead of learning the full turnscore, but that didn't perform as well.</d-footnote> Through experimentation we found that trying to predict the food reward of each ant had a regularizing effect on the training of the value function. Huber loss was used for the weight update.</p>

        <h4>A2C</h4>
        <p>Finally, we use A2C in order to fine tune the policy and value networks. We obtain the game action \(A\), and estimated reward \(R\) using the equation:
            $$
            \forall_{c \in (\text{cells in S})} A[c] =
            \begin{cases}
                \pi(s_c, \theta_{\pi}), & \text{if}\ c\ \text{contains an ant} \\
                0, & \text{otherwise}
            \end{cases}
            $$
            $$
            R_{score} = \frac{1}{N} \cdot \sum\limits_{i=1}^N Q_{score}(s_i, a_i) \text{ where N is the number of ants in state} \ S
            $$
            $$
            R_{food} = \sum\limits_{i=1}^N Q_{food}(s_i, a_i) \text{ where N is the number of ants in state} \ S
            $$
        </p>

        <p>Each turn is played using \(A\). The Q-values obtained by the value network predict both the turn score, and the food gathered, hence we obtain two different rewards \(R_{food}\) and \(R_{score}\)<d-footnote>As mentioned in footnote 1, using two rewards had better performance than just using the score. We suspect this is because \(R_{food}\) is a short term reward, while \(R_{score}\) is a long term reward. Using both together balances the total reward.</d-footnote>. The net loss for the critic is given by:
            $$ L = MSE(R_{food}, \text{actual food collected}) + MSE(R_{score}, \text{actual turn score}) $$
        </p>

        <h4>Results</h4>
        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">Map1</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">88</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td class="right-border">146</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>130</td>
                </tr>
                <tr>
                    <th class="right-border">Map2</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained A2C bot against the baseline bots on <code>Map1</code> and <code>Map2</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>

        <d-figure>
            <video id="poster-vid" src="assets/videos/a2c-compare.mp4" autoplay mute controls loop preload="auto"></video>
            <figcaption>Comparison of the untrained policy network (on left) and trained policy network (right) vs the <code>GreedyBot</code> (blue player). Through training we are able to go from a swift defeat to a major victory.</figcaption>
        </d-figure>

        <p>Our bot gets major wins against the <code>Random</code>, <code>Hunter</code>, and <code>Greedy</code> bots. However, despite the wins, it still gets completely destroyed against the <code>Winner</code> bot.</p>

        <p>There is still a long way to go to be able to beat an A* implementation of the bot. One other major cause of concern is that the current trained bots learn to play only on the specified maps. On testing with unseen randomly generated maps, the A2C bot is unable to conclusively beat even the <code>Random</code> bot. This indicates that work must be done to generate generic features of the game state which will remain valid across all maps.</p>

    </d-article>
    <d-appendix>
        <h3>Acknowledgement</h3>
        <p>We would like to thank our Professor, Dr. Joseph Lim, for giving us the opportunity to work on this project. We also take this opportunity to express our gratitude to Dr. Joseph Lim and our assigned T.A., Youngwoon Lee for guiding us at every step on the way.</p>
        <!--
        <h3>Author Contributions</h3>
        <p>Likhit - Custom Ants Gym environment, Implementation of Supervised Training using A2C</p>
        <p>Anurima, Apoorva, Arun and Dhanya - Implemenation of Deep Q-Learning using Experience Replay and Target Network, Double Deep
        Q Learning, Deep Q- Learning using RNN
        -->
        <d-citation-list></d-citation-list>
    </d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
</html>
