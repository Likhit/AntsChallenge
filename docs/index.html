<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <link rel="stylesheet" type="text/css" href="index.css">

    <script type="text/javascript" src="https://distill.pub/template.v2.js"></script>
    <script type="text/javascript" src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script src="index.js"></script>

    <title>Using Deep Reinforcement Learning to play Ants</title>
</head>
<body>
    <d-front-matter>
        <script type="text/json">{
            "title": "Using Deep Reinforcement Learning to play Ants",
            "description": "Ants is a multiplayer turn based capture-the-hill board game. Here we attempt to train a bot to play the game using deep RL.",
            "authors": [{
                "author": "Anurima Padwal",
                "authorURL": "https://www.linkedin.com/in/anurima-padwal-433260135/",
                "affiliation": ""
            }, {
                "author": "Apoorva KH",
                "authorURL": "https://www.linkedin.com/in/apoorvakh/",
                "affiliation": ""
            }, {
                "author": "Arunkumar Rajendran",
                "authorURL": "https://www.linkedin.com/in/arunkumar-rajendran/",
                "affiliation": ""
            }, {
                "author": "Dhanya Raghu",
                "authorURL": "https://www.linkedin.com/in/dhanya-raghu/",
                "affiliation": ""
            }, {
                "author": "Likhit Dharmapuri",
                "authorURL": "https://www.linkedin.com/in/likhitd",
                "affiliation": ""
            }]
        }</script>

    </d-front-matter>

    <d-title>
        <h1>Using Deep Reinforcement Learning to Play Ants</h1>
        <p>Ants is a multiplayer turn based capture-the-hill board game. Here we attempt to train a bot to play the game using deep reinforcement learning.</p>
        <d-figure class="l-screen">
            <video id="poster-vid" src="assets/videos/winnergame.mp4" autoplay mute controls loop preload="auto" playbackRate="3"></video>
            <figcaption>
                An example of the ants game with 7 players. Each player starts with two ant hills in this game.
            </figcaption>
        </d-figure>
    </d-title>
    <d-article>
        <p>The <a href="http://ants.aichallenge.org/">“Ants” AI Challenge</a> was an artificial intelligence programming contest conducted by the University of Waterloo in Fall 2011. This contest requires the creation of an AI agent that controls a colony of ants to fight other colonies for dominance in a competitive environment.</p>
        <p>The game itself is a multiplayer strategy game set on a plot of dirt and water. Each player starts with a set of ant hills and one ant on each hill. By gathering food that spawns randomly on the board, players can increase the number of ants under their control. The final objective of the game is to capture the enemy's ant hill by placing the player's ant on top of the enemy hill.</p>

        <h3>Game Rules</h3>
        <p><em>Ants</em> is a turn based game. Players can only see the portion of the board within the vicinity of their own ants. At each turn, every player submits moves each of their ants on the board in one of the four directions: north, east, south or west. Once all players have made their submissions the game executes the turn in the following phases:</p>

        <ol id="game-phases">
            <li><em>move</em> all ants in the specified direction (ants from the same team are killed if they collide)</li>

            <li><em>attack</em> enemy ants within range of each other</li>

            <li><em>raze</em> any ant hill with an enemy ant positioned on it</li>

            <li><em>spawn</em> more ants at un-razed player ant hills if the player has enough food reserves</li>

            <li><em>gather</em> food within range of an ant (food disappears if it is within range of multiple enemy ants)</li>
        </ol>

        <p>After execution of all phases, each player receives a new board state, and the game proceeds to the next turn.</p>

        <h3>Challenges</h3>
        <p>Since each ant on the board is moved at the end of every turn, the player is in effect controlling multiple agents (one agent per ant). Not only that, the number of agents is not fixed &mdash; as the game progresses, the number of ants in on the board will increase. Multi-agent RL is a challenging problem which is still in its nascent stages of research.</p>
        <p>All entries to the competition so far have been implementations of min-max and the A* algorithm. There has been no attempt so far to use a trained bot to play the game.</p>
        <p>This allows us to work on something novel and challenging. At the same time the computationally simple nature of the game provides a feasible problem to work on.</p>

        <h2>Problem Formulation</h2>
        <p>For the sake of this project we limit ourselves to boards with only two players, and only one ant hill per player. We also limit ourselves to playing on the following two maps.</p>

        <d-figure>
            <div id="maps">
                <img src="assets/images/map1.png" alt="map1">
                <img src="assets/images/map2.png" alt="map2">
            </div>
            <figcaption>The maps used for both training and testing throughout this project. The left map is of size <code>43 x 39</code>, while the map on the right is of size <code>110 x 114</code>.</figcaption>
        </d-figure>

        <p>Henceforth, the map on the left will be called <code>Map1</code>, while the map on the right will be call <code>Map2</code> in the rest of this post.</p>

        <p>A common vocabulary used to define problems in the RL landscape are:</p>
        <ul>
            <li><em>State Space:</em> The set of all states that the game can take, and</li>
            <li><em>Action Space:</em> The set of all possible actions that can be ever taken</li>
        </ul>
        <p>This vocabulary is also used to describe problems abstractly by <a href="https://gym.openai.com/">OpenAI Gym</a>, the popular toolkit for developing and comparing reinforcement learning algorithms. Therefore, we formulated our game using the same vocabulary in order to leverage existing work on RL.</p>

        <h3>Game State</h3>
        <p>Given a game board of height <code>H</code> and width <code>W</code>, there are a number aspects on the board (like visible area, food positions, etc.) which the game state needs to capture. Using an image of the game board would be unnecessarily complicated. Instead, the game board can be described as a byte tensor <code>S</code> of the shape <code>(7, H, W)</code> where the seven channels describe different board features.</p>

        <d-figure class="l-page">
            <div id="state-diag">
                <img src="assets/images/full.png">
                <ul>
                    <li id="visibility-channel">Visibility Channel: <code>True</code> if the cell is visible to the agent</li>
                    <li id="land-channel">Land Channel: <code>True</code> if the the cell is land, <code>False</code> if water</li>
                    <li id="food-channel">Food Channel: <code>True</code> if the cell has food</li>
                    <li id="ant-hill-channel">Ant Hill Channel: <code>PLAYER NUMBER</code> if the cell contains an ant hill</li>
                    <li id="player-ant-channel">Player Ant Channel: <code>True</code> if the cell contains one of the player's ants</li>
                    <li id="enemy-ant-channel">Enemy Ant Channel: <code>True</code> if the cell contains one of the enemy's ants</li>
                </ul>
                <img id="channel-img" src="assets/images/visibility.png">
            </div>
            <figcaption>State description of example game board from the perspective of the orange player. Black color represents a value of <code>1</code> / <code>True</code>, while white depicts <code>0</code> / <code>False</code>; except in the visibility channel in which white depicts the visible cells.</figcaption>
        </d-figure>

        <h3>Action Space</h3>
        <p>In order to avoid dealing with a changing action space (which changes with the number of ants on the board), we instead use a constant action space tensor <code>A</code> of the shape <code>(H, W)</code>. The values in <code>A[h, w]</code> can be one of <code>north (1)</code>, <code>south (2)</code>, <code>east (3)</code>, <code>west (4)</code>, and <code>none (0)</code>. The values describe the direction in which the ant in cell <code>(h, w)</code> should move. <code>none (0)</code> is used in cells without any ant, and for ants which shouldn't move.</p>

        <d-figure>
            <img src="assets/images/actionspace.jpeg">
            <figcaption>An example action for the given board state of the orange player. Blue depicts <code>north</code>, pink depicts <code>south</code>, yellow <code>east</code> and green <code>west</code>. All the white cells (majority of the map) are <code>none</code></figcaption>
        </d-figure>

        <h3>Rewards</h3>
        <p>A reward of <code>1</code> is obtained every time an enemy ant hill is razed, and a penalty of <code>-1</code> is incurred when the player's ant hill is razed. Such a reward is very sparse since most games go on for a few hundred turns. In order to augment the score, we have developed a number of other reward functions.</p>

        <ol id="game-phases">
            <li><em>Food : </em> Rewards movement towards the food positively.</li>

            <li><em>Score : </em> Improvement in score is positively rewarded.</li>

            <li><em>Death : </em> Death of ants is negatively rewarded.</li>

            <li><em>Collision : </em> Collision of two ants of the same team is penalized.</li>

            <li><em>Obstacles : </em> Encourages ants to move away from obstacles (i.e water).</li>
        </ol>

        <p>The new reward functions all have the ability to provide rewards for each ant rather than the whole board state. Depending on the mode of training, we might choose to use this distributed reward, or the total state reward.</p>

        <h2>Approaches</h2>
        <p>We primarily attempted to train bots using multiple variations of the DQN and A2C algorithm. But before we detail the approaches, we must first specify a way to test the performance of our trained bots.</p>

        <h3>Baselines</h3>
        <p>In order to test the performance of our bots we played our bots against a number of simpler bots.

        <br>
        {Table for each bot: Random, Hunter, Greedy, and Xanthis}
        <br>
        {Videos for gameplay}
        <br>

        In order to discount for randomness of the game (eg. food spawn rate, etc.) we run the trained bots multiple times against each one of the baseline bots.</p>

        <h3>DQN</h3>
        <p>Reinforcement Learning is a technique in which an agent attempts to learn an optimal policy based on its interactions with the environment. At each time step, the agent observes a state \(S\), chooses an action \(A\) to perform on the state, obtains a reward \(R\), and transitions to a new state \(\hat{S}\). Q-learning is an approach in which we incrementally estimate the “value” of performing an action \(A\) in state \(S\) by updating the following rule:
            $$ Q(S, A) = Q(S, A) + \alpha \cdot (R + \gamma \cdot max_{\hat{A}} Q(\hat{S}, \hat{A}) - Q(S, A)) $$
        </p>

        <p>Here, \(Q(S, A)\) represents the quality of taking an action \(A\) in state \(S\). Q-learning can be extended to deep reinforcement learning frameworks by using a neural network as a function approximator \(Q(S, A|\theta)\) where \(\theta\) represents the weights of the network that parametrize the Q values.
        </p>

        <p>
            The state space of the game is passed through a CNN to extract the features of the state space, following which we learn the probabilities of the five actions for each cell in which our agent ant is present using Q learning.
        We build a deep network to learn the values of Q for deciding the action to be taken, i.e., the policy function. Q-learning learns the action-value function Q(s, a): how good it is to take an action at a particular state. We select an action based on our Q-values using exploration and exploitation techniques.

        </p>
        <p>
            The target for calculating the loss is calculated by taking the maximum of the action values possible for the next state.
            Further, we use Huber loss for training our DQN model. The weights of the neural network are updated by minimizing the loss function:
            $$
            L(&delta;) =
            \begin{cases}
            \frac{1}{2} \cdot (y - \hat{y})^2, & \text{if } |(y - \hat{y})| &lt; &delta; \\
            &delta; \cdot (y - \hat{y}) - \frac{&delta;}{2} & \text{otherwise}
            \end{cases}
            $$
        </p>

        <p>We use experience replay to further improve the training of the model.</p>


        <br>
        <img src="assets/images/DQN.jpg" height = "80%>

        <h4>Result</h4>
        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">Simple</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">88</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td class="right-border">146</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>130</td>
                </tr>
                <tr>
                    <th class="right-border">Collision</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
                <tr>
                    <th class="right-border">Die</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
                <tr>
                    <th class="right-border">Double</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained DQN bots against the baseline bots on <code>Map1</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>

        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">Simple</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">88</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td class="right-border">146</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>130</td>
                </tr>
                <tr>
                    <th class="right-border">Collision</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
                <tr>
                    <th class="right-border">Die</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
                <tr>
                    <th class="right-border">Double</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained DQN bots against the baseline bots on <code>Map2</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>
        <br>
        {Video of side by side game play against random bot. Left video is DQN without any training. Right is trained DQN.}
        <br>
        {Describe the result, what is good, and what is bad. And why bad (difficulty in training reason).}


        <h3>Supervised Training + A2C</h3>
        <p>The <em>A2C</em> or Advantage Actor Critic algorithms are a set of policy gradient methods. The agent has two parts, the <strong>actor</strong> i.e. the policy network which learns the probability with which each action should be taken, and the <strong>critic</strong> i.e. the value function which estimates the value of each state. The critic is used to compare against the actual reward obtained by the agent; the difference which is the error is then used to update both the actor as well as the critic.</p>

        <p>So far, we have been trying to learn the actions for all ants on the board in one shot. As described earlier, this is an extremely hard function to learn. In order to offset this problem, we instead try to train a network which learns the best action for each ant separately.</p>

        <p>In order to do so, given a game state \(S\), we decompose \(S\) into a sub-state \(s_i\) for each ant \(i\) in \(S\). This is done by taking a square centered around each any \(i\).</p>

        <d-figure>
            <img src="assets/images/substates.png">
            <figcaption>Sub-states created for the orange player. A small box of <code>15x15</code> cells is taken centered around each ant.</figcaption>
        </d-figure>

        <p>Once we have our sub states, we define our policy \(\pi(s_i, \theta_{\pi})\) which returns an action \(a_i\) for the ant \(i\). Similarly, our value function is defined as \(Q(s_i, a_i)\) which returns the reward \(r_i\) obtained by performing action \(a_i\) on ant \(i\).</p>

        <p>We attempt to learn the policy and value function using the following networks.</p>
        {Insert network diag} <br>

        <p>The training technique described in this section is heavily inspired from the AlphaGo paper. The training of the bot goes through two phases.</p>

        <h4>Supervised training</h4>
        <p>In this stage, we learn a policy and value function from generated training data. The training data is generated by playing the <code>RandomBot</code>, <code>HunterBot</code>, <code>GreedyBot</code>, and the <code>WinnerBot</code> against each other multiple times on <code>Map1</code>, and <code>Map2</code>. For each turn of every game, we store the current state \(S\), action taken \(A\), and reward earned \(R\) by each player. The total data collected is broken an \(80/20\) train/test split.</p>

        <h5>Policy network training</h5>
        <p>The network is trained only on samples generated by the <code>WinnerBot</code>. Given the state we decompose it into a set of sub-states, and try to predict the action taken taken by the winner bot for the ant in that sub-state. We use <code>cross-entropy</code> loss for the weight update.</p>

        <h5>Value network training</h5>
        <p>In order to train the value network, we use both the <code>FoodReward</code> as well as the game scores. We do not restrict the training data in any way. We obtain the game score for non-final turns by discounting the final score obtained in a gameplay by a factor or \(0.999\). This discounted score is called the turn score. For each sub-state of a state, the network tries to predict both the food gathered by the ant in the sub-state, as well as the turn score for the whole state. <d-footnote>We also tried to learn \(\frac{1}{N} \cdot turnscore\) (where \(N\) is number of ants) instead of learning the full turnscore, but that didn't perform as well.</d-footnote> Through experimentation we found that trying to predict the food reward of each ant had a regularizing effect on the training of the value function. Huber loss was used for the weight update.</p>

        <h4>A2C</h4>
        <p>Finally, we use A2C in order to fine tune the policy and value networks. We obtain the game action \(A\), and estimated reward \(R\) using the equation:
            $$
            \forall_{c \in (\text{cells in S})} A[c] =
            \begin{cases}
                \pi(s_c, \theta_{\pi}), & \text{if}\ c\ \text{contains an ant} \\
                0, & \text{otherwise}
            \end{cases}
            $$
            $$
            R_{score} = \frac{1}{N} \cdot \sum\limits_{i=1}^N Q_{score}(s_i, a_i) \text{ where N is the number of ants in state} \ S
            $$
            $$
            R_{food} = \sum\limits_{i=1}^N Q_{food}(s_i, a_i) \text{ where N is the number of ants in state} \ S
            $$
        </p>

        <p>Each turn is played using \(A\). The Q-values obtained by the value network predict both the turn score, and the food gathered, hence we obtain two different rewards \(R_{food}\) and \(R_{score}\)<d-footnote>As mentioned in footnote 1, using two rewards had better performance than just using the score. We suspect this is because \(R_{food}\) is a short term reward, while \(R_{score}\) is a long term reward. Using both together balances the total reward.</d-footnote>. The net loss for the critic is given by:
            $$ L = MSE(R_{food}, \text{actual food collected}) + MSE(R_{score}, \text{actual turn score}) $$
        </p>

        <h4>Results</h4>
        <d-figure>
            <table>
                <tr>
                    <th rowspan="2" class="right-border"></th>
                    <th colspan="4" class="right-border">RandomBot</th>
                    <th colspan="4" class="right-border">HunterBot</th>
                    <th colspan="4">GreedyBot</th>
                </tr>
                <tr>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th class="right-border">#Turns</th>
                    <th>Wins</th>
                    <th>Losses</th>
                    <th>Ties</th>
                    <th>#Turns</th>
                </tr>
                <tr>
                    <th class="right-border">Map1</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">88</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td class="right-border">146</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>130</td>
                </tr>
                <tr>
                    <th class="right-border">Map2</th>
                    <td>100%</td>
                    <td>0%</td>
                    <td>0%</td>
                    <td class="right-border">431</td>
                    <td>80%</td>
                    <td>0%</td>
                    <td>20%</td>
                    <td class="right-border">475</td>
                    <td>90%</td>
                    <td>0%</td>
                    <td>10%</td>
                    <td>509</td>
                </tr>
            </table>
            <figcaption>The table depicts the percentage of wins, losses, ties, and the average number of turns per game for the trained A2C bot against the baseline bots on <code>Map1</code> and <code>Map2</code>. Each player pairing was played 10 times on each map.</figcaption>
        </d-figure>

        <d-figure>
            <video id="poster-vid" src="assets/videos/a2c-compare.mp4" autoplay mute controls loop preload="auto"></video>
            <figcaption>Comparison of the untrained policy network (on left) and trained policy network (right) vs the <code>GreedyBot</code> (blue player). Through training we are able to go from a swift defeat to a major victory.</figcaption>
        </d-figure>

        <p>Our bot gets major wins against the <code>Random</code>, <code>Hunter</code>, and <code>Greedy</code> bots. However, despite the wins, it still gets completely destroyed against the <code>Winner</code> bot.</p>

        <p>There is still a long way to go to be able to beat an A* implementation of the bot. One other major cause of concern is that the current trained bots learn to play only on the specified maps. On testing with unseen randomly generated maps, the A2C bot is unable to conclusively beat even the <code>Random</code> bot. This indicates that work must be done to generate generic features of the game state which will remain valid across all maps.</p>

    </d-article>
    <d-appendix>
        <h3>Acknowledgement</h3>
        <p>We would like to thank our Professor, Dr. Joseph Lim, for giving us the opportunity to work on this project. We also take this opportunity to express our gratitude to Dr. Joseph Lim and our assigned T.A., Youngwoon Lee for guiding us at every step on the way.</p>
        <!--
        <h3>Author Contributions</h3>
        <p>Who did what</p>
        -->
        <d-citation-list></d-citation-list>
    </d-appendix>
</body>
</html>
